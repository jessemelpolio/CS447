{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSy-sfxOsclS"
      },
      "source": [
        "# CS447 - Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIc7zIy7qrIt"
      },
      "source": [
        "In this part of assignment 2 we'll be building a machine learning model to detect the sentiment of movie reviews using the Stanford Sentiment Treebank([SST])(http://ai.stanford.edu/~amaas/data/sentiment/) dataset. First we will import all the required libraries. We highly recommend that you finish the PyTorch Tutorials [ 1 ](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html),[ 2 ](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html),[ 3 ](https://github.com/yunjey/pytorch-tutorial) before starting this assignment. \n",
        "\n",
        "After finishing this assignment you will able to answer the following questions:\n",
        "* How to write Dataloaders in Pytorch?\n",
        "* How to build dictionaries and vocabularies for Deep Nets?\n",
        "* How to use Embedding Layers in Pytorch?\n",
        "* How to use a Convolutional Neural Network for  sentiment analysis?\n",
        "* How to build various recurrent models (LSTMs and GRUs) for sentiment analysis?\n",
        "* How to use packed_padded_sequences for sequential models?\n",
        "\n",
        "Please make sure that you have selected \"GPU\" as the Hardware accelerator from Runtime -> Change runtime type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgzwGooD2bpy"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyCOvTRQ1nb-",
        "outputId": "bfc92259-0c39-4e46-9e93-b612a02e1767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Don't import any other libraries\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.optim as optim\n",
        "from torchtext import data, datasets\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if __name__=='__main__':\n",
        "    print('Using device:', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHbJ1-aDsWCG"
      },
      "source": [
        "## Download dataset\n",
        "First we will download the dataset using [torchtext](https://torchtext.readthedocs.io/en/latest/index.html), which is a package that supports NLP for PyTorch. The following command will get you 3 objects `train_data`, `val_data` and `test_data`. To access the data:\n",
        "\n",
        "*   To access list of textual tokens - `train_data[0].text`\n",
        "*   To access label - `train_data[0].label`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfX3bNby8FYL",
        "outputId": "5aa2ec83-027e-43c3-cf65-c5c224ca9267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    train_data, val_data, test_data = datasets.SST.splits(data.Field(tokenize = 'spacy'), data.LabelField(dtype = torch.float), filter_pred=lambda ex: ex.label != 'neutral')\n",
        "\n",
        "    print('{:d} train and {:d} test samples'.format(len(train_data), len(test_data)))\n",
        "\n",
        "    print('Sample text:', train_data[0].text)\n",
        "    print('Sample label:', train_data[0].label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading trainDevTestTrees_PTB.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "trainDevTestTrees_PTB.zip: 100%|██████████| 790k/790k [00:01<00:00, 743kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n",
            "6920 train and 1821 test samples\n",
            "Sample text: ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '`', '`', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean', '-', 'Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.']\n",
            "Sample label: positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kfg8RcyskyU"
      },
      "source": [
        "# 1. Define the Dataset Class (4 points)\n",
        "\n",
        "In the following cell, we will define the dataset class. You need to implement the following functions: \n",
        "\n",
        "\n",
        "*   ` build_dictionary() ` - creates the dictionaries `ixtoword` and `wordtoix`. Converts all the text of all examples, in the form of text ids and stores them in `textual_ids`. If a word is not present in your dictionary, it should use `<unk>`. Use the hyperparameter `THRESHOLD` to control which words appear in the dictionary, based on their frequency in the training data. Note that a word’s frequency should be `>=THRESHOLD` to be included in the dictionary. Also make sure that `<end>` should be at idx 0, and `<unk>` should be at idx 1\n",
        "\n",
        "*   ` get_label() ` - This function should return the value `1` if the label in the dataset is `positive`, and should return `0` if it is `negative`. The data type for the returned item should be `torch.LongTensor`\n",
        "\n",
        "*   ` get_text() ` - This function should pad the review with `<end>` character up to a length of `MAX_LEN` if the length of the text is less than the `MAX_LEN`. If length is more than `MAX_LEN` then it should only return the first `MAX_LEN` words. This function should also return the original length of the review. The data type for the returned items should be `torch.LongTensor`. Note that the text returned is a list of indices of the words from your `wordtoix` mapping\n",
        "\n",
        "*   ` __len__() ` - This function should return the total length (int value) of the dataset i.e. the number of sentences\n",
        "\n",
        "*   ` __getitem__() ` - This function should return the padded text, the length of the text (without the padding) and the label. The data type for all the returned items should be `torch.LongTensor`. You will use the ` get_label() ` and ` get_text() ` functions here\n",
        "\n",
        "NOTE: Don't forget to convert all text to lowercase!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1irMn3LX2YDB"
      },
      "source": [
        "THRESHOLD = 10\n",
        "MAX_LEN = 60\n",
        "UNK = '<unk>'\n",
        "END = '<end>'\n",
        "\n",
        "class TextDataset(data.Dataset):\n",
        "    def __init__(self, examples, split, ixtoword=None, wordtoix=None, THRESHOLD=THRESHOLD, MAX_LEN=MAX_LEN):\n",
        "        self.examples = examples\n",
        "        self.split = split\n",
        "        self.ixtoword = ixtoword\n",
        "        self.wordtoix = wordtoix\n",
        "        self.THRESHOLD = THRESHOLD\n",
        "        self.MAX_LEN = MAX_LEN\n",
        "\n",
        "        self.build_dictionary()\n",
        "        self.vocab_size = len(self.ixtoword)\n",
        "        \n",
        "        self.textual_ids = []\n",
        "        self.labels = []\n",
        "        ##### TODO #####\n",
        "        # textual_ids contains list of word ids as per wordtoix for all sentences\n",
        "        #   Replace words out of vocabulary with id of UNK token\n",
        "        # labels is a list of integer labels (0 or 1) for all sentences\n",
        "        for sentences in self.examples:\n",
        "          seq = []\n",
        "          for word in sentences.text:\n",
        "            if word in self.wordtoix:\n",
        "              seq.append(self.wordtoix[word])\n",
        "            else:\n",
        "              seq.append(self.wordtoix[UNK])\n",
        "          # seq.append(self.wordtoix[END])\n",
        "          self.textual_ids.append(seq)\n",
        "          self.labels.append(sentences.label)\n",
        "    \n",
        "    def build_dictionary(self): \n",
        "        # This is built only from train dataset and then reused in test dataset \n",
        "        # by passing ixtoword and wordtoix from train dataset to __init__() when creating test dataset\n",
        "        # which is done under 'Initialize the Dataloader' section\n",
        "        if self.split.lower() != 'train':\n",
        "            return\n",
        "        \n",
        "        # END should be at idx 0. UNK should be at idx 1 \n",
        "        self.ixtoword = {0:END, 1:UNK}\n",
        "        self.wordtoix = {END:0, UNK:1}\n",
        "\n",
        "        ##### TODO #####\n",
        "        # Count the frequencies of all words in the training data (self.examples)\n",
        "        # Assign idx (starting from 2) to all words having word_freq >= THRESHOLD\n",
        "        idx = 2\n",
        "        cnt_word = defaultdict(int)\n",
        "        for sentences in self.examples:\n",
        "          for word in sentences.text:\n",
        "            cnt_word[word.lower()] += 1\n",
        "       \n",
        "        for key in cnt_word.keys():\n",
        "          if cnt_word[key] >= self.THRESHOLD:\n",
        "            self.ixtoword[idx] = key\n",
        "            self.wordtoix[key] = idx\n",
        "            idx += 1\n",
        "        return\n",
        "\n",
        "    def get_label(self, index):\n",
        "        ##### TODO #####\n",
        "        # This function should return the value 1 if the label is positive, and 0 if it is negative for sentence at index `index`\n",
        "        # The data type for the returned item should be torch.LongTensor\n",
        "        if self.labels[index] == 'positive':\n",
        "          label = 1\n",
        "        else:\n",
        "          label = 0\n",
        "        label = torch.LongTensor([label])\n",
        "        label = torch.squeeze(label)\n",
        "        return label\n",
        "     \n",
        "    def get_text(self, index):\n",
        "        ##### TODO #####\n",
        "        # This function should pad the text with END token uptil a length of MAX_LEN if the length of the text is less than the MAX_LEN\n",
        "        #   If length is more than MAX_LEN then only return the first MAX_LEN words\n",
        "        # This function should also return the original length of the review\n",
        "        # The data type for the returned items should be torch.LongTensor\n",
        "        # Note that the text returned is a list of indices of the words from your wordtoix mapping\n",
        "        seq = self.textual_ids[index]\n",
        "        text = []\n",
        "        text_len = torch.LongTensor([len(seq)])\n",
        "        text_len = torch.squeeze(text_len)\n",
        "        for idx in range(self.MAX_LEN):\n",
        "          if idx >= len(seq):\n",
        "            text.append(0)\n",
        "          else:\n",
        "            text.append(seq[idx])\n",
        "        text = torch.LongTensor(text)\n",
        "        return text, text_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        ##### TODO #####\n",
        "        # This function should return the number of sentences (int value) in the dataset\n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text, text_len = self.get_text(index)\n",
        "        label = self.get_label(index)\n",
        "\n",
        "        return text, text_len, label"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSxpGXj6ml9N",
        "outputId": "d0cd61a6-5961-4b40-cb20-0a15ae03b6e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    # Sample item\n",
        "    Ds = TextDataset(train_data, 'train')\n",
        "    print('vocab_size:', Ds.vocab_size)\n",
        "\n",
        "    text, text_len, label = Ds[0]\n",
        "    print('text:', text)\n",
        "    print('text_len:', text_len)\n",
        "    print('label:', label)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 1469\n",
            "text: tensor([ 1,  1,  4,  1,  5,  6,  2,  1,  1,  8,  9, 10, 10,  1, 11, 12, 13, 14,\n",
            "         8, 15,  5, 16, 17,  1, 18,  1, 19,  1,  1, 20,  1, 21,  1,  1,  1, 22,\n",
            "         1,  1, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0])\n",
            "text_len: tensor(39)\n",
            "label: tensor(1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6oP0IpRZveu"
      },
      "source": [
        "# Some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zpJmsJiE-gq"
      },
      "source": [
        "##### Do not modify this\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Count number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch\n",
        "    output: Tensor [batch_size, n_classes]\n",
        "    labels: LongTensor [batch_size]\n",
        "    \"\"\"\n",
        "    preds = output.argmax(dim=1) # find predicted class\n",
        "    correct = (preds == labels).sum().float() # convert into float for division \n",
        "    acc = correct / len(labels)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DJc8Rxzt3JS"
      },
      "source": [
        "# Train your Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD-Jj2rUFOzr"
      },
      "source": [
        "##### Do not modify this\n",
        "\n",
        "def train_model(model, num_epochs, data_loader, optimizer, criterion):\n",
        "    print('Training Model...')\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for texts, text_lens, labels in data_loader:\n",
        "            texts = texts.to(device) # shape: [batch_size, MAX_LEN]\n",
        "            text_lens = text_lens.to(device) # shape: [batch_size]\n",
        "            labels = labels.to(device) # shape: [batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(texts, text_lens)\n",
        "            acc = accuracy(output, labels)\n",
        "            \n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        print('[TRAIN]\\t Epoch: {:2d}\\t Loss: {:.4f}\\t Accuracy: {:.2f}%'.format(epoch+1, epoch_loss/len(data_loader), 100*epoch_acc/len(data_loader)))\n",
        "    print('Model Trained!\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-OJbZ72t6Yq"
      },
      "source": [
        "# Evaluate your Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTiiYDZIF--7"
      },
      "source": [
        "##### Do not modify this\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    print('Evaluating performance on Test dataset...')\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    all_predictions = []\n",
        "    for texts, text_lens, labels in data_loader:\n",
        "        texts = texts.to(device)\n",
        "        text_lens = text_lens.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        output = model(texts, text_lens)\n",
        "        acc = accuracy(output, labels)\n",
        "        all_predictions.append(output.argmax(dim=1))\n",
        "        \n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    print('[TEST]\\t Loss: {:.4f}\\t Accuracy: {:.2f}%'.format(epoch_loss/len(data_loader), 100*epoch_acc/len(data_loader)))\n",
        "    predictions = torch.cat(all_predictions)\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_4FFhulaAod"
      },
      "source": [
        "# 2. Build your Convolutional Neural Network Model (3 points)\n",
        "In the following we provide you the class to build your model. We provide some parameters that we expect you to use in the initialization of your sequential model. Do not change these parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ztuy2hUaAof"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, out_channels, filter_heights, stride, num_classes, dropout, pad_idx):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        ##### TODO #####\n",
        "        # Create an embedding layer (https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "        #   to represent the words in your vocabulary. You can vary the dimensionality of the embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Define multiple Convolution layers (nn.Conv2d) with filter (kernel) size [filter_height, embed_size] based on your different filter_heights.\n",
        "        #   Input channels will be 1 and output channels will be `out_channels` (these many different filters will be trained for each convolution layer)\n",
        "        #   If you want, you can have a list of modules inside nn.ModuleList\n",
        "        # Note that even though your conv layers are nn.Conv2d, we are doing a 1d convolution since we are only moving the filter in one direction\n",
        "        #\n",
        "        # You can vary the number of output channels, filter heights, and stride\n",
        "        self.conv1 = nn.Conv2d(1, out_channels, (filter_heights[0], embed_size), stride=stride)\n",
        "        self.conv2 = nn.Conv2d(1, out_channels, (filter_heights[1], embed_size), stride=stride)\n",
        "        self.conv3 = nn.Conv2d(1, out_channels, (filter_heights[2], embed_size), stride=stride)\n",
        "\n",
        "        # Define a linear layer (nn.Linear) that consists of num_classes (2 in our case) units \n",
        "        #   and takes as input the concatenated output for all cnn layers (out_channels * num_of_cnn_layers units)\n",
        "        self.linear = nn.Linear(out_channels * 3, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, texts, text_lens):\n",
        "        \"\"\"\n",
        "        texts: LongTensor [batch_size, MAX_LEN]\n",
        "        text_lens: LongTensor [batch_size] - you might not even need to use this\n",
        "        \n",
        "        Returns output: Tensor [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        ##### TODO #####\n",
        "\n",
        "        # Pass texts through your embedding layer to convert from word ids to word embeddings\n",
        "        # texts: [batch_size, MAX_LEN, embed_size]\n",
        "        em = self.embedding(texts)\n",
        "        # input to conv should have 1 channel. Take a look at torch's unsqueeze() function\n",
        "        # texts [batch_size, 1, MAX_LEN, embed_size]\n",
        "        x1 = self.conv1(em.unsqueeze(1))\n",
        "        x2 = self.conv2(em.unsqueeze(1))\n",
        "        x3 = self.conv3(em.unsqueeze(1))\n",
        "\n",
        "        # Pass these texts to each of your cnn and compute their output as follows:\n",
        "        #   Your cnn output will have shape [batch_size, out_channels, *, 1] where * depends on filter_height and stride\n",
        "        #   Convert to shape [batch_size, out_channels, *] (see torch's squeeze() function)\n",
        "        x1 = x1.squeeze(3)\n",
        "        x2 = x2.squeeze(3)\n",
        "        x3 = x3.squeeze(3)\n",
        "\n",
        "        #   Apply non-linearity on it (F.relu() is a commonly used one. Feel free to try others)\n",
        "        x1 = F.relu(x1)\n",
        "        x2 = F.relu(x2)\n",
        "        x3 = F.relu(x3)\n",
        "        #   Take the max value across last dimension to have shape [batch_size, out_channels]\n",
        "        x1 = torch.max(x1, 2).values\n",
        "        x2 = torch.max(x2, 2).values\n",
        "        x3 = torch.max(x3, 2).values\n",
        "\n",
        "        # Concatenate (torch.cat) outputs from all your cnns [batch_size, (out_channels*num_of_cnn_layers)]\n",
        "        #\n",
        "        x = torch.cat((x1,x2,x3), 1)\n",
        "        # Let's understand what you just did:\n",
        "        #   Since each cnn is of different filter_height, it will look at different number of words at a time\n",
        "        #     So, a filter_height of 3 means your cnn looks at 3 words (3-grams) at a time and tries to extract some information from it\n",
        "        #   Each cnn will learn `out_channels` number of different features from the words it sees at a time\n",
        "        #   Then you applied a non-linearity and took the max value for all channels\n",
        "        #     You are essentially trying to find important n-grams from the entire text\n",
        "        # Everything happens on a batch simultaneously hence you have that additional batch_size as the first dimension\n",
        "\n",
        "        # optionally apply a dropout if you want to (You will have to initialize an nn.Dropout layer in __init__)\n",
        "\n",
        "        # Pass your concatenated output through your linear layer and return its output ([batch_size, num_classes])\n",
        "\n",
        "        ##### NOTE: Do not apply a sigmoid or softmax to the final output - done in evaluation method!\n",
        "        output = self.linear(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YcKGX6nn9OL"
      },
      "source": [
        "## Initialize the Dataloader\n",
        "We initialize the training and testing dataloaders using the Dataset classes we create for both training and testing. Make sure you use the same vocabulary for both the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2QYl334n9ON"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    BATCH_SIZE = 64 # Feel free to try other batch sizes\n",
        "\n",
        "    ##### Do not modify this\n",
        "    Ds = TextDataset(train_data, 'train')\n",
        "    train_loader = torch.utils.data.DataLoader(Ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "    test_Ds = TextDataset(test_data, 'test', Ds.ixtoword, Ds.wordtoix)\n",
        "    test_loader = torch.utils.data.DataLoader(test_Ds, batch_size=1, shuffle=False, num_workers=1, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYHwCx4coNn4"
      },
      "source": [
        "## Training and Evaluation for CNN Model\n",
        "\n",
        "We first train your model using the training data. Feel free to play around with the hyperparameters. We recommend **you write code to save your model** [(save/load model tutorial)](https://pytorch.org/tutorials/beginner/saving_loading_models.html) as colab connections are not permanent and it can get messy if you'll have to train your model again and again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z6cS6D8oNn9",
        "outputId": "d80ea15e-34ab-4a55-e67c-c0fc8844fb98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    ##### Do not modify this\n",
        "    VOCAB_SIZE = Ds.vocab_size\n",
        "    NUM_CLASSES = 2\n",
        "    PAD_IDX = 0\n",
        "    \n",
        "    # Hyperparameters (Feel free to play around with these)\n",
        "    EMBEDDING_DIM = 1024\n",
        "    DROPOUT = 0\n",
        "    OUT_CHANNELS = 64\n",
        "    FILTER_HEIGHTS = [1, 3, 5] # [3 different filter sizes - unigram, bigram, trigram in this case. Feel free to try other n-grams as well]\n",
        "    STRIDE = 1\n",
        "    model = CNN(VOCAB_SIZE, EMBEDDING_DIM, OUT_CHANNELS, FILTER_HEIGHTS, STRIDE, NUM_CLASSES, DROPOUT, PAD_IDX)\n",
        "\n",
        "    # put your model on device\n",
        "    model = model.to(device)\n",
        "    \n",
        "    print('The model has {:,d} trainable parameters'.format(count_parameters(model)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,094,658 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoeyQL4PoNoH"
      },
      "source": [
        "if __name__=='__main__':    \n",
        "    LEARNING_RATE = 5e-4 # Feel free to try other learning rates\n",
        "\n",
        "    # Define your loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Define your optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPOs1FifoNoN",
        "outputId": "75531b1c-1465-4115-e99a-e98eb773f533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "if __name__=='__main__':    \n",
        "    N_EPOCHS = 8 # Feel free to change this\n",
        "    \n",
        "    # train model for N_EPOCHS epochs\n",
        "    train_model(model, N_EPOCHS, train_loader, optimizer, criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model...\n",
            "[TRAIN]\t Epoch:  1\t Loss: 0.6514\t Accuracy: 62.43%\n",
            "[TRAIN]\t Epoch:  2\t Loss: 0.4528\t Accuracy: 79.15%\n",
            "[TRAIN]\t Epoch:  3\t Loss: 0.2866\t Accuracy: 89.92%\n",
            "[TRAIN]\t Epoch:  4\t Loss: 0.1559\t Accuracy: 96.17%\n",
            "[TRAIN]\t Epoch:  5\t Loss: 0.0818\t Accuracy: 98.61%\n",
            "[TRAIN]\t Epoch:  6\t Loss: 0.0479\t Accuracy: 99.19%\n",
            "[TRAIN]\t Epoch:  7\t Loss: 0.0305\t Accuracy: 99.36%\n",
            "[TRAIN]\t Epoch:  8\t Loss: 0.0233\t Accuracy: 99.45%\n",
            "Model Trained!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z718w8e0oNoS",
        "outputId": "57ed74f9-acb9-44d0-921f-fe11f41ca949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "##### Do not modify this\n",
        "\n",
        "if __name__=='__main__':    \n",
        "    # Compute test data accuracy\n",
        "    predictions_cnn = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    # Convert tensor to numpy array \n",
        "    # This will be saved to your Google Drive below and you will be submitting this file to gradescope\n",
        "    predictions_cnn = predictions_cnn.cpu().data.detach().numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating performance on Test dataset...\n",
            "[TEST]\t Loss: 0.6267\t Accuracy: 76.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRCFvjwDthiA"
      },
      "source": [
        "# 3. Build your Recurrent Neural Network Model (3 points)\n",
        "In the following we provide you the class to build your model. We provide some parameters that we expect you to use in the initialization of your sequential model. Do not change these parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nc_HxbP6klI"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes, num_layers, bidirectional, dropout, pad_idx):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        ##### TODO #####\n",
        "        # Create an embedding layer (https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) \n",
        "        #   to represent the words in your vocabulary. You can vary the dimensionality of the embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Create a recurrent network (nn.LSTM or nn.GRU) with batch_first = False\n",
        "        # You can vary the number of hidden units, directions, layers, and dropout\n",
        "        self.rnn = nn.LSTM(embed_size, self.hidden_size, self.num_layers, bidirectional=bidirectional)\n",
        "        \n",
        "        # Define a linear layer (nn.Linear) that consists of num_classes (2 in our case) units \n",
        "        #   and takes as input the output of the last timestep (in the bidirectional case: the output of the last timestep\n",
        "        #   of the forward direction, concatenated with the output of the last timestep of the backward direction)\n",
        "        self.linear = nn.Linear(self.hidden_size*2, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, texts, text_lens):\n",
        "        \"\"\"\n",
        "        texts: LongTensor [batch_size, MAX_LEN]\n",
        "        text_lens: LongTensor [batch_size]\n",
        "        \n",
        "        Returns output: Tensor [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        ##### TODO #####\n",
        "\n",
        "        # permute texts for sentence_len first dimension\n",
        "        # texts: [MAX_LEN, batch_size]\n",
        "        texts = texts.T\n",
        "        # Pass texts through your embedding layer to convert from word ids to word embeddings\n",
        "        # texts: [MAX_LEN, batch_size, embed_size]\n",
        "        em = self.embedding(texts)\n",
        "        \n",
        "        # Pack texts into PackedSequence using nn.utils.rnn.pack_padded_sequence\n",
        "        em = nn.utils.rnn.pack_padded_sequence(em, [MAX_LEN]*text_lens.shape[0])\n",
        "\n",
        "        # Pass the pack through your recurrent network\n",
        "        output, (h_n, c_n) = self.rnn(em)\n",
        "        # print(h_n.view(self.num_layers, 2, text_lens.shape[0], self.hidden_size))\n",
        "        h_n = h_n.view(self.num_layers, 2, text_lens.shape[0], self.hidden_size)\n",
        "        h_n_forward = h_n[-1, 0]\n",
        "        h_n_backward = h_n[-1, 1]\n",
        "\n",
        "        # Take output of the last timestep of the last layer for all directions and concatenate them (see torch.cat())\n",
        "        #   depends on whether your model is bidirectional\n",
        "        # Your concatenated output will have shape [batch_size, num_dirs*hidden_size]\n",
        "        lin_in = torch.cat((h_n_forward, h_n_backward), 1)\n",
        "        # optionally apply a dropout if you want to (You will have to initialize an nn.Dropout layer in __init__)\n",
        "\n",
        "        # Pass your concatenated output through your linear layer and return its output ([batch_size, num_classes])\n",
        "        output = self.linear(lin_in)\n",
        "        ##### NOTE: Do not apply a sigmoid or softmax to the final output - done in evaluation method!\n",
        "\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baD8lYAytdTV"
      },
      "source": [
        "## Initialize the Dataloader\n",
        "We initialize the training and testing dataloaders using the Dataset classes we create for both training and testing. Make sure you use the same vocabulary for both the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCzNm8LDM5aT"
      },
      "source": [
        "if __name__=='__main__':\n",
        "    BATCH_SIZE = 32 # Feel free to try other batch sizes\n",
        "\n",
        "    ##### Do not modify this\n",
        "    Ds = TextDataset(train_data, 'train')\n",
        "    train_loader = torch.utils.data.DataLoader(Ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "    test_Ds = TextDataset(test_data, 'test', Ds.ixtoword, Ds.wordtoix)\n",
        "    test_loader = torch.utils.data.DataLoader(test_Ds, batch_size=1, shuffle=False, num_workers=1, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhNl48rew3KW"
      },
      "source": [
        "## Training and Evaluation for Sequential Model\n",
        "\n",
        "We first train your model using the training data. Feel free to play around with the hyperparameters. We recommend **you write code to save your model** [(save/load model tutorial)](https://pytorch.org/tutorials/beginner/saving_loading_models.html) as colab connections are not permanent and it can get messy if you'll have to train your model again and again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA-UairGErap",
        "outputId": "6c2a26f6-c371-41eb-f60a-20b6568c0e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    ##### Do not modify this\n",
        "    VOCAB_SIZE = Ds.vocab_size\n",
        "    NUM_CLASSES = 2\n",
        "    PAD_IDX = 0\n",
        "\n",
        "    # Hyperparameters (Feel free to play around with these)\n",
        "    EMBEDDING_DIM = 1024\n",
        "    DROPOUT = 0\n",
        "    BIDIRECTIONAL = True\n",
        "    HIDDEN_DIM = 256\n",
        "    N_LAYERS = 4\n",
        "    \n",
        "    model = RNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "    # put your model on device\n",
        "    model = model.to(device)\n",
        "    \n",
        "    print('The model has {:,d} trainable parameters'.format(count_parameters(model)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 8,861,698 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em6Rs58OlJ3Z"
      },
      "source": [
        "if __name__=='__main__':    \n",
        "    LEARNING_RATE = 5e-4 # Feel free to try other learning rates\n",
        "\n",
        "    # Define your loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Define your optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR8Wckf0l2G7",
        "outputId": "4f7c958d-7649-4b9f-ada9-942dbc7ef2c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "if __name__=='__main__':    \n",
        "    N_EPOCHS = 10 # Feel free to change this\n",
        "    \n",
        "    # train model for N_EPOCHS epochs\n",
        "    train_model(model, N_EPOCHS, train_loader, optimizer, criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model...\n",
            "[TRAIN]\t Epoch:  1\t Loss: 0.6592\t Accuracy: 60.26%\n",
            "[TRAIN]\t Epoch:  2\t Loss: 0.4876\t Accuracy: 77.45%\n",
            "[TRAIN]\t Epoch:  3\t Loss: 0.3413\t Accuracy: 86.08%\n",
            "[TRAIN]\t Epoch:  4\t Loss: 0.2278\t Accuracy: 91.35%\n",
            "[TRAIN]\t Epoch:  5\t Loss: 0.1503\t Accuracy: 94.44%\n",
            "[TRAIN]\t Epoch:  6\t Loss: 0.0824\t Accuracy: 97.14%\n",
            "[TRAIN]\t Epoch:  7\t Loss: 0.0657\t Accuracy: 97.82%\n",
            "[TRAIN]\t Epoch:  8\t Loss: 0.0560\t Accuracy: 97.93%\n",
            "[TRAIN]\t Epoch:  9\t Loss: 0.0354\t Accuracy: 98.73%\n",
            "[TRAIN]\t Epoch: 10\t Loss: 0.0339\t Accuracy: 98.87%\n",
            "Model Trained!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYon4AbHl5_M",
        "outputId": "591e28bd-0176-4400-9333-07ca10aa82d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "##### Do not modify this\n",
        "\n",
        "if __name__=='__main__':    \n",
        "    # Compute test data accuracy\n",
        "    predictions_rnn = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    # Convert tensor to numpy array \n",
        "    # This will be saved to your Google Drive below and you will be submitting this file to gradescope\n",
        "    predictions_rnn = predictions_rnn.cpu().data.detach().numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating performance on Test dataset...\n",
            "[TEST]\t Loss: 1.1929\t Accuracy: 75.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WQAV6O2xHvS"
      },
      "source": [
        "# Saving test results to your Google drive for submission.\n",
        "You will save the `predictions_rnn.txt` and `predictions_cnn.txt` with your test data results. Make sure you do not **shuffle** the order of the `test_data` or the autograder will give you a bad score.\n",
        "\n",
        "You will submit the following files to the autograder on the gradescope :\n",
        "1.   Your `predictions_cnn.txt` of test data results\n",
        "1.   Your `predictions_rnn.txt` of test data results\n",
        "2.   Your code of this notebook. You can do it by clicking `File`-> `Download .py` - make sure the name of the downloaded file is `HW2.py`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abbbMNi8X_ai",
        "outputId": "7380737d-858f-418c-f102-341bd5c3188f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "##### Do not modify this\n",
        "\n",
        "if __name__=='__main__':\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    np.savetxt('drive/My Drive/predictions_cnn.txt', predictions_cnn, delimiter=',')\n",
        "    np.savetxt('drive/My Drive/predictions_rnn.txt', predictions_rnn, delimiter=',')\n",
        "\n",
        "    print('Files saved successfully!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Files saved successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_PDga7qPFfA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}