{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gau9xEXMGY8s"
   },
   "source": [
    "# CS 447 HW4 - Using Attention for Neural Machine Translation\n",
    "In this notebook we are going to perform machine translation using a deep learning based approach and attention mechanism.\n",
    "\n",
    "Specifically, we are going to train a sequence to sequence model for Spanish to English translation.  We will use Sequence to Sequence Models for this Assignment. In this assignment you only need tto implement the encoder and decoder, we implement all the data loading for you. Please **refer** to the following resources for more details:\n",
    "\n",
    "1.   https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "2.   https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "3. https://arxiv.org/pdf/1409.0473.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9mf5x3zHp1A"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import unicodedata\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvR1U28i6Itb"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_WR8vEGMQyS"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    \"\"\"Normalizes latin chars with accent to their canonical decomposition\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "# Preprocessing the sentence to add the start, end tokens and make them lower-case\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n",
    "    w = re.sub(r'[\" \"]+', ' ', w)\n",
    "\n",
    "    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "def pad_sequences(x, max_len):\n",
    "    padded = np.zeros((max_len), dtype=np.int64)\n",
    "    if len(x) > max_len:\n",
    "        padded[:] = x[:max_len]\n",
    "    else:\n",
    "        padded[:len(x)] = x\n",
    "    return padded\n",
    "\n",
    "\n",
    "def preprocess_data_to_tensor(dataframe, inp_lang, targ_lang):\n",
    "    # Vectorize the input and target languages\n",
    "    input_tensor = [[inp_lang.word2idx[s if s in inp_lang.vocab else '<unk>'] for s in es.split(' ')] for es in dataframe['es'].values.tolist()]\n",
    "    target_tensor = [[targ_lang.word2idx[s if s in targ_lang.vocab else '<unk>'] for s in eng.split(' ')] for eng in dataframe['eng'].values.tolist()]\n",
    "\n",
    "    # calculate the max_length of input and output tensor for padding\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    print('max_length_inp: {}, max_length_tar: {}'.format(max_length_inp, max_length_tar))\n",
    "\n",
    "    # pad all the sentences in the dataset with the max_length\n",
    "    input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
    "    target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]\n",
    "\n",
    "    return input_tensor, target_tensor, max_length_inp, max_length_tar\n",
    "\n",
    "\n",
    "def train_val_split(input_tensor, target_tensor):\n",
    "    \"\"\"Creating training and test/val sets using an 80-20 split\"\"\"\n",
    "    total_num_examples = len(input_tensor)\n",
    "    num_val = int(total_num_examples/5)\n",
    "    num_train = total_num_examples - num_val\n",
    "    \n",
    "    input_tensor_train, input_tensor_val = input_tensor[:num_train], input_tensor[num_train:]\n",
    "    target_tensor_train, target_tensor_val = target_tensor[:num_train], target_tensor[num_train:]\n",
    "\n",
    "    assert len(input_tensor_train) == num_train\n",
    "    assert len(target_tensor_train) == num_train\n",
    "    assert len(input_tensor_val) == num_val\n",
    "    assert len(target_tensor_val) == num_val\n",
    "\n",
    "    return input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val\n",
    "\n",
    "\n",
    "def sort_batch(X, y, lengths):\n",
    "    \"\"\"sort batch function to be able to use with pad_packed_sequence\"\"\"\n",
    "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
    "    X = X[indx]\n",
    "    y = y[indx]\n",
    "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XH8nu0ojQpt"
   },
   "source": [
    "# Download The Data\n",
    "\n",
    "Here we will download the translation data. We will learn a model to translate Spanish to English.\n",
    "\n",
    "NOTE: Comment all lines in the below cell before submitting your code. \n",
    "These statement is only valid in a Notebook file and not in a Python .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtyBFlMKIg7g"
   },
   "outputs": [],
   "source": [
    "# NOTE: Comment all 3 lines in the below cell before submitting your code to gradescope.\n",
    "if __name__ == '__main__':\n",
    "    !wget http://www.manythings.org/anki/spa-eng.zip\n",
    "    !unzip -o spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WNjizED6eFI"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    lines = open('spa.txt', encoding='UTF-8').read().strip().split('\\n')\n",
    "    total_num_examples = 30000 \n",
    "    original_word_pairs = [[w for w in l.split('\\t')][:2] for l in lines[:total_num_examples]]\n",
    "    data = pd.DataFrame(original_word_pairs, columns=['eng', 'es'])\n",
    "    print(data) # visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mud7HbQUMUHB"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Make sure YOU only run this once - if you run it twice it will mess up the data so you will have run the above cell again\n",
    "    # Now we do the preprocessing using pandas and lambdas\n",
    "    data['eng'] = data.eng.apply(lambda w: preprocess_sentence(w))\n",
    "    data['es'] = data.es.apply(lambda w: preprocess_sentence(w))\n",
    "    print(data) # visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHJw_CyykmMp"
   },
   "source": [
    "# Vocabulary Class\n",
    "\n",
    "We create a class here for managing our vocabulary as we did in HW2. In this HW, we have a separate class for the vocabulary as we need 2 different vocabularies - one for English and one for Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1h4Q21azMW-T"
   },
   "outputs": [],
   "source": [
    "class Vocab_Lang():\n",
    "    def __init__(self, vocab):\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>'}\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 2 # +2 because of <pad> and <unk> token\n",
    "            self.idx2word[index + 2] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a2c34aFnPOP"
   },
   "source": [
    "# Dataloader for our Encoder and Decoder\n",
    "\n",
    "We prepare the dataloader and make sure the dataloader returns the source sentence, target sentence and the length of the source sentenc sampled from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c797aZAWMzrW"
   },
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.length = torch.LongTensor([np.sum(1 - np.equal(x, 0)) for x in X])\n",
    "        self.data = torch.LongTensor(X)\n",
    "        self.target = torch.LongTensor(y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        x_len = self.length[index]\n",
    "        return x, y, x_len\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaH022cEy03q"
   },
   "source": [
    "# Train your model\n",
    "\n",
    "You will train your model here.\n",
    "*   Pass the source sentence and their corresponding lengths into the encoder\n",
    "*   Creating the decoder input using <start> tokens\n",
    "*   Now we find out the decoder outputs conditioned on the previous predicted word usually, but in our training we use teacher forcing. Read more about teacher forcing at https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "*   We evaluate on the test set.\n",
    "*   In this evaluation, instead of using the concept of teacher forcing, we use the prediction of the decoder as the input to the decoder for the sequence of outputs.\n",
    "\n",
    "## Bleu Score Calculation for evaluation\n",
    "\n",
    "Read more about Bleu Score at :\n",
    "\n",
    "1.   https://en.wikipedia.org/wiki/BLEU\n",
    "2.   https://www.aclweb.org/anthology/P02-1040.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bp2rKJY4NIzx"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
    "    mask = real.ge(1).float()\n",
    "    \n",
    "    loss_ = F.cross_entropy(pred, real) * mask \n",
    "    return torch.mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9663UYJNMgv"
   },
   "outputs": [],
   "source": [
    "def train_model(encoder, decoder, dataset, optimizer, targ_lang, device, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        n_batch = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        for inp, targ, inp_len in tqdm(dataset):\n",
    "            n_batch += 1\n",
    "            loss = 0\n",
    "            \n",
    "            xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
    "            enc_output, enc_hidden = encoder(xs.to(device), lens)\n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            # use teacher forcing - feeding the target as the next input (via dec_input)\n",
    "            dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
    "        \n",
    "            # run code below for every timestep in the ys batch\n",
    "            for t in range(1, ys.size(1)):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input.to(device), dec_hidden.to(device), enc_output.to(device))\n",
    "                loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
    "                dec_input = ys[:, t].unsqueeze(1)\n",
    "        \n",
    "            batch_loss = (loss / int(ys.size(1)))\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            ### update model parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "        ### TODO: Save checkpoint for model (optional)\n",
    "        print('Epoch:{:2d}/{}\\t Loss:{:.4f} ({:.2f}s)'.format(epoch + 1, n_epochs, total_loss / n_batch, time.time() - start))\n",
    "\n",
    "    print('Model trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1emvSSo0NRbQ"
   },
   "outputs": [],
   "source": [
    "def get_reference_candidate(target, pred, targ_lang):\n",
    "    reference = list(target)\n",
    "    reference = [targ_lang.idx2word[s] for s in np.array(reference[1:])]\n",
    "    candidate = list(pred)\n",
    "    candidate = [targ_lang.idx2word[s] for s in np.array(candidate[1:])]\n",
    "    return reference, candidate\n",
    "\n",
    "\n",
    "def evaluate_model(encoder, decoder, val_dataset, target_tensor_val, max_length_tar, targ_lang, device):\n",
    "    batch_size = val_dataset.batch_size\n",
    "    n_batch = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    final_output = torch.zeros((len(target_tensor_val), max_length_tar))\n",
    "    target_output = torch.zeros((len(target_tensor_val), max_length_tar))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (inp, targ, inp_len) in enumerate(val_dataset):\n",
    "            n_batch += 1\n",
    "            loss = 0\n",
    "            xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
    "            enc_output, enc_hidden = encoder(xs.to(device), lens)\n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * batch_size)\n",
    "            curr_output = torch.zeros((ys.size(0), ys.size(1)))\n",
    "            curr_output[:, 0] = dec_input.squeeze(1)\n",
    "\n",
    "            for t in range(1, ys.size(1)): # run code below for every timestep in the ys batch\n",
    "                predictions, dec_hidden, _ = decoder(dec_input.to(device), dec_hidden.to(device), enc_output.to(device))\n",
    "                loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
    "                dec_input = torch.argmax(predictions, dim=1).unsqueeze(1)\n",
    "                curr_output[:, t] = dec_input.squeeze(1)\n",
    "        \n",
    "            final_output[batch*batch_size:(batch+1)*batch_size] = curr_output\n",
    "            target_output[batch*batch_size:(batch+1)*batch_size] = targ\n",
    "            batch_loss = (loss / int(ys.size(1)))\n",
    "            total_loss += batch_loss\n",
    "\n",
    "        print('Loss {:.4f}'.format(total_loss / n_batch))\n",
    "    \n",
    "    # Compute Bleu scores\n",
    "    bleu_1 = 0.0\n",
    "    bleu_2 = 0.0\n",
    "    bleu_3 = 0.0\n",
    "    bleu_4 = 0.0\n",
    "\n",
    "    smoother = SmoothingFunction()\n",
    "    save_reference = []\n",
    "    save_candidate = []\n",
    "    for i in range(len(target_tensor_val)):\n",
    "        reference, candidate = get_reference_candidate(target_output[i], final_output[i], targ_lang)\n",
    "    \n",
    "        bleu_1 += sentence_bleu(reference, candidate, weights=(1,), smoothing_function=smoother.method1)\n",
    "        bleu_2 += sentence_bleu(reference, candidate, weights=(1/2, 1/2), smoothing_function=smoother.method1)\n",
    "        bleu_3 += sentence_bleu(reference, candidate, weights=(1/3, 1/3, 1/3), smoothing_function=smoother.method1)\n",
    "        bleu_4 += sentence_bleu(reference, candidate, weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=smoother.method1)\n",
    "\n",
    "        save_reference.append(reference)\n",
    "        save_candidate.append(candidate)\n",
    "    \n",
    "    bleu_1 = bleu_1/len(target_tensor_val)\n",
    "    bleu_2 = bleu_2/len(target_tensor_val)\n",
    "    bleu_3 = bleu_3/len(target_tensor_val)\n",
    "    bleu_4 = bleu_4/len(target_tensor_val)\n",
    "\n",
    "    # bleu_1 = corpus_bleu(save_reference, save_candidate, weights=(1,), smoothing_function=smoother.method1)\n",
    "    # bleu_2 = corpus_bleu(save_reference, save_candidate, weights=(1/2, 1/2), smoothing_function=smoother.method1)\n",
    "    # bleu_3 = corpus_bleu(save_reference, save_candidate, weights=(1/3, 1/3, 1/3), smoothing_function=smoother.method1)\n",
    "    # bleu_4 = corpus_bleu(save_reference, save_candidate, weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=smoother.method1)\n",
    "\n",
    "    print('BLEU 1-gram: %f' % (bleu_1))\n",
    "    print('BLEU 2-gram: %f' % (bleu_2))\n",
    "    print('BLEU 3-gram: %f' % (bleu_3))\n",
    "    print('BLEU 4-gram: %f' % (bleu_4))\n",
    "\n",
    "    return save_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umSF6nAka57s"
   },
   "source": [
    "# Use Pretrained Embeddings\n",
    "\n",
    "The embedding used in HW2 was initialized with random vectors and learnt while training. Here we will use the FastText embedding method proposed by Facebook's AI Research lab to improve our translation result. Particularly, we will use an implementation from the gensim library to train the embedding of our corpus.\n",
    "\n",
    "Read more about FastText and gensim liberary:\n",
    "https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA4lG_4fqL9U"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW4jAq_CsN7z"
   },
   "source": [
    "# Train FastText Embeddings (Implement This)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChRig0iPrpUF"
   },
   "outputs": [],
   "source": [
    "def compute_FastText_embeddings(pd_dataframe, embedding_dim=256):\n",
    "    \"\"\"\n",
    "    Given dataset (pd.DataFrame as used in the beginning), train FastText embeddings\n",
    "    Return FastText trained model and embeddings vectors (np array [2 + vocab_size, embedding_dim])\n",
    "    \"\"\"\n",
    "    print('Computing FastText Embeddings...')\n",
    "    sentences = [sen.split() for sen in pd_dataframe]\n",
    "    \n",
    "    ##### TODO:\n",
    "    # 1. create FastText model to learn `embedding_dim` sized embedding vectors\n",
    "    model = FastText(size=embedding_dim)\n",
    "\n",
    "    # 2. build vocab from sentences\n",
    "    model.build_vocab(sentences=sentences)\n",
    "\n",
    "    # 3. train model on sentences for 10 epochs\n",
    "    model.train(sentences=sentences,total_examples=len(sentences),epochs=10)\n",
    "    \n",
    "\n",
    "    # 4. The sentences that we used to train the embedding don't contain '<pad>', or '<unk>' \n",
    "    # so add two all-zero or random rows in the beginning of the embedding numpy array for '<pad>' and '<unk>'\n",
    "    embedding_vec = model.wv.vectors # np.array [vocab_size, embedding_dim]\n",
    "    embedding_vec = np.insert(embedding_vec, [0], [[0], [0]], axis=0) # np.array [2 + vocab_size, embedding_dim]\n",
    "\n",
    "    return model, embedding_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrgMtGxHoqFi"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # HYPERPARAMETERS (No need to experiemnt with other hyperparameters as these seem to work fine)\n",
    "    BATCH_SIZE = 64\n",
    "    EMBEDDING_DIM = 256\n",
    "    UNITS = 512\n",
    "    LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtlsUrvzetQO"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model_eng, embedding_eng = compute_FastText_embeddings(data['eng'], EMBEDDING_DIM)\n",
    "    model_es, embedding_es = compute_FastText_embeddings(data['es'], EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6UsDOA2c-G6"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    inp_lang = Vocab_Lang(model_es.wv.vocab)\n",
    "    targ_lang = Vocab_Lang(model_eng.wv.vocab)\n",
    "\n",
    "    input_tensor, target_tensor, max_length_inp, max_length_tar = preprocess_data_to_tensor(data, inp_lang, targ_lang)\n",
    "    input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_val_split(input_tensor, target_tensor)\n",
    "\n",
    "    # create train and val datasets\n",
    "    train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
    "    train_dataset = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "\n",
    "    val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
    "    val_dataset = DataLoader(val_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWO5ptloc-HL"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    inp, targ, inp_len = train_dataset.dataset[0]\n",
    "    print('Input:', inp)\n",
    "    print('Target:', targ)\n",
    "    print('Input Length:', inp_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENeT1fj_2f8t"
   },
   "source": [
    "# Encoder Model (Implement This)\n",
    "\n",
    "First we build a simple encoder model, which will be very similar to what you did in MP2. But instead of using a fully connected layer as the output, you should the return the output of your recurrent net (GRU/LSTM) as well as the hidden output. They are used in the decoder later.\n",
    "\n",
    "NOTE: Initialize your embedding layer with those embeddings. Refer to nn.Embedding.from_pretrained in https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Sx4QQd3M4XK"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, pretrained_emb, vocab_size, embedding_dim, enc_units, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        ##### TODO:\n",
    "        # Convert pretrained_emb from np.array to torch.FloatTensor\n",
    "        self.pretrained_emb = torch.from_numpy(pretrained_emb)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Initialize embedding layer with pretrained_emb\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_emb)\n",
    "        \n",
    "        # Initialize a single directional LSTM/GRU with 1 layers and batch_first=False\n",
    "        self.rnn = nn.LSTM(embedding_dim, enc_units, num_layers)\n",
    "        \n",
    "    def forward(self, x, lens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [max_len, batch_size]\n",
    "            lens: [batch_size]\n",
    "\n",
    "        Returns:\n",
    "            unpacked_output: [max_len, batch_size, enc_units]\n",
    "            hidden_state: [1, batch_size, enc_units]\n",
    "        \n",
    "        Pseudo-code:\n",
    "        - Pass x through an embedding layer\n",
    "        - Make sure x is correctly packed before the recurrent net \n",
    "        - Pass it through the recurrent net\n",
    "        - Make sure the output is unpacked correctly\n",
    "        - Return hidden states from the recurrent net (for last time step) and the unpacked output\n",
    "        \"\"\"\n",
    "        x = self.embedding(x) # [max_len, batch_size, embedding_dim]\n",
    "        \n",
    "        ##### TODO:\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lens)\n",
    "        output, (hidden_state, c_n) = self.rnn(packed_x)\n",
    "        unpacked_output = nn.utils.rnn.pad_packed_sequence(output)[0] \n",
    "        return unpacked_output, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKwsEWpK2mcT"
   },
   "source": [
    "# Decoder Model (Implement This)\n",
    "We will implement a Decoder model which uses an attention mechanism. We will implement the decoder as provided in https://arxiv.org/pdf/1409.0473.pdf. **Please read** the links provided above first, at the start of this assignment for review. The pseudo-code for your implementation should be somewhat as follows:\n",
    "\n",
    "\n",
    "1.   The input is put through an encoder model which gives us the encoder output of shape *(max_length, batch_size, enc_units)* and the encoder hidden state of shape *(1, batch_size, enc_units)*. \n",
    "2.   Using the output your encoder you will calculate the score and subsequently the attention using following equations : \n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "3. Once you have calculated this attention vector, you pass the original input x through a embedding layer. The output of this embedding layer is concatenated with the attention vector which is passed into your RNN.\n",
    "\n",
    "4. Finally you pass the output of your RNN into a fully connected layer with an output size same as that vocab, to see the probability of the most possible word.\n",
    "\n",
    "NOTE: Initialize your embedding layer with those embeddings. Refer to nn.Embedding.from_pretrained in https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw84M2LPM-PC"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, pretrained_emb, vocab_size, embedding_dim, dec_units, enc_units, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        ##### TODO:\n",
    "        # Convert pretrained_emb from np.array to torch.FloatTensor\n",
    "        self.pretrained_emb = torch.from_numpy(pretrained_emb)\n",
    "\n",
    "        # Initialize embedding layer with pretrained_emb\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.pretrained_emb)\n",
    "        \n",
    "        # Layers to compute score based on the formula you pick\n",
    "        # We have tested the model using Bahdanau's additive style formula\n",
    "        #   but feel free to try the Loung's style also and see if you can get better performance\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.w1 = nn.Linear(enc_units, enc_units)\n",
    "        self.w2 = nn.Linear(enc_units, enc_units)\n",
    "        self.V = nn.Linear(enc_units, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Initialize a single directional LSTM/GRU with 1 layers and batch_first=True\n",
    "        # NOTE: input to your rnn would be a concatenation of two types of vectors\n",
    "        self.rnn = nn.LSTM(embedding_dim+enc_units, dec_units, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(dec_units, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, 1]\n",
    "            hidden: [1, batch_size, enc_units]\n",
    "            enc_output: [max_len, batch_size, enc_units]\n",
    "\n",
    "        Returns:\n",
    "            fc_out: [batch_size, vocab_size]\n",
    "            hidden_state [1, batch_size, dec_units]\n",
    "            attention_weights: [batch_size, max_len, 1]\n",
    "\n",
    "        Pseudo-code:\n",
    "        - Calculate the score using the formula shown above using encoder output and hidden output. \n",
    "            Note h_t is the hidden output of the decoder and h_s is the encoder output in the formula\n",
    "        - Calculate the attention weights using softmax and passing through V - which can be implemented as a fully connected layer\n",
    "        - Finally find c_t which is a context vector where the shape of context_vector should be (batch_size, hidden_size)\n",
    "        - You need to unsqueeze the context_vector for concatenating with x_embedding as listed in Point 3 above\n",
    "        - Pass this concatenated tensor to the RNN and follow as specified in Point 4 above\n",
    "        - Return (i)  output of your fc layer (takes output of your RNN as its input (might have to reshape it first))\n",
    "        -        (ii) hidden states from the recurrent net (for last time step)\n",
    "        -        (ii) attention weights\n",
    "        \"\"\"\n",
    "        hidden = hidden.permute(1, 0, 2) # [batch_size, 1, enc_units]\n",
    "        enc_output = enc_output.permute(1, 0, 2) # [batch_size, max_len, enc_units]\n",
    "\n",
    "        ##### TODO:\n",
    "        score = self.V(self.tanh(self.w1(hidden)+self.w2(enc_output))) # [batch_size, max_len, 1]\n",
    "        attention_weights = self.softmax(score)      \n",
    "        context_vector = torch.sum(attention_weights*enc_output, 1) # [batch_size, enc_units]\n",
    "        c_t = torch.unsqueeze(context_vector, 1) # [batch_size, 1, enc_units]\n",
    "        x_embedding = self.embedding(x)\n",
    "        cx = torch.cat((c_t, x_embedding), 2)\n",
    "        output, (hidden_state, _) = self.rnn(cx)\n",
    "        output = torch.squeeze(output)\n",
    "        fc_out = self.fc(output)\n",
    "\n",
    "        return fc_out, hidden_state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjbsUkcpNK9W"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    vocab_inp_size = len(inp_lang.word2idx)\n",
    "    vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "    encoder = Encoder(embedding_es, vocab_inp_size, EMBEDDING_DIM, UNITS).to(device)\n",
    "    decoder = Decoder(embedding_eng, vocab_tar_size, EMBEDDING_DIM, UNITS, UNITS).to(device)\n",
    "\n",
    "    model_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(model_params, lr=LEARNING_RATE)\n",
    "\n",
    "    print('Encoder and Decoder models initialized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQKYT5w3n82V"
   },
   "source": [
    "# Train and evaluate your model here\n",
    "We expect your scores to be in the range of for full credit for each of the 4 BLEU scores individually. No partial credit :( \n",
    "\n",
    "*   BLEU-1 > 0.145\n",
    "*   BLEU-2 > 0.030\n",
    "*   BLEU-3 > 0.020\n",
    "*   BLEU-4 > 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPosimvgdx_O"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_model(encoder, decoder, train_dataset, optimizer, targ_lang, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Doqxb5jnpk9z"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    save_candidate = evaluate_model(encoder, decoder, val_dataset, target_tensor_val, max_length_tar, targ_lang, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYKSWRO93Mjz"
   },
   "source": [
    "# Save results to your Google Drive for Submission\n",
    "You need to submit this **results.pkl** file to the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2Jnt32ItMA7"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    pickle.dump(save_candidate, open('drive/My Drive/results.pkl', 'wb'))\n",
    "    print('results.pkl saved to your Google Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3TbkOknPGzt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
